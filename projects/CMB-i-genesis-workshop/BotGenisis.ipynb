{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgJDVy9DPL8IH4u0h59uHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMB-i/S25E02-AI_ML_Workshop/blob/master/BotGenisis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. importing the required libraries"
      ],
      "metadata": {
        "id": "ZDmBRR9brwLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cHEHeKOqW4y"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install peft\n",
        "!pip install auto-gptq\n",
        "!pip install optimum\n",
        "!pip install bitsandbytes\n",
        "!pip install gradio\n",
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n"
      ],
      "metadata": {
        "id": "0TMs1PW7rTYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Loading the model and defining its parameters"
      ],
      "metadata": {
        "id": "r9uKo1bcrnyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name= \"BAAI/bge-small-en-v1.5\")\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 256\n",
        "Settings.chunk_overlap = 25\n"
      ],
      "metadata": {
        "id": "8-0h-EOsruJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. reading and store data\n"
      ],
      "metadata": {
        "id": "xx_gsLCBsHmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loader = PyMuPDFReader()\n",
        "pages = pdf_loader.load_data(\"/content/BotGenesis/MATRIX_CRITICAL_REVIEW_cinema&philosophy_SE22UCSE106.pdf\")\n",
        "\n"
      ],
      "metadata": {
        "id": "z8oWKbxstIkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pages))"
      ],
      "metadata": {
        "id": "dZ1gg7ZgwAI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(pages)"
      ],
      "metadata": {
        "id": "bxJzf_-VwIFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 13\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index = index,\n",
        "    similarity_top_k= top_k,\n",
        ")"
      ],
      "metadata": {
        "id": "_cqo2YHuwPXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever= retriever,\n",
        "    node_postprocessors= [SimilarityPostprocessor(similarity_cutoff=0.5)]\n",
        ")"
      ],
      "metadata": {
        "id": "H1hfbj7Mwl3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who embodies the quote IGNORANCE IS BLISS\"\n",
        "response = query_engine.query(query)"
      ],
      "metadata": {
        "id": "CQxurIKsxN6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Context: \\n\"\n",
        "for i in range(top_k):\n",
        "  context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
        "print(context)"
      ],
      "metadata": {
        "id": "kAJyFQAYzFj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(response.source_nodes))  # Check how many chunks were retrieved"
      ],
      "metadata": {
        "id": "BNcsLdfyzaU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load fine-tuned model from hub\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n",
        "model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "ytqjNDikz8f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (no context)\n",
        "intstructions_string = f\"\"\"EnigmaGPT, functioning as a chatbot to answer questions on your personalized text material, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–EnigmaGPT'. \\\n",
        "EnigmaGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please respond to the following comment.\n",
        "\"\"\"\n",
        "prompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n",
        "\n",
        "comment = \"Who embodies the quote IGNORANCE IS BLISS?\" #Replace with the same question as used in query above\n",
        "\n",
        "prompt = prompt_template(comment)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "H0857_Pw04sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure pad token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set PAD token\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()  # ✅ This ensures the model behaves correctly for inference\n",
        "\n",
        "# Tokenize input with padding and truncation\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512  # Set an appropriate length\n",
        ")\n",
        "\n",
        "# Generate output with proper attention mask\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(\"cuda\"),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(\"cuda\"),\n",
        "    max_new_tokens=280\n",
        ")\n",
        "\n",
        "# Decode and print output\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "5745toHe4xix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt (with context)\n",
        "prompt_template_w_context = lambda context, comment: f\"\"\"[INST]EnigmaGPT, functioning as a chatbot to answer questions on your personalized text material, communicates in clear, accessible language, escalating to technical depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '–EnigmaGPT'. \\\n",
        "EnigmaGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "{context}\n",
        "Please respond to the following comment. Use the context above if it is helpful.\n",
        "\n",
        "{comment}\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template_w_context(context, comment)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "KnJD24aH417t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "CxCfWoUH7plY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages = pdf_loader.load_data(\"/content/BotGenesis/MATRIX_CRITICAL_REVIEW_cinema&philosophy_SE22UCSE106.pdf\")\n",
        "index = VectorStoreIndex.from_documents(pages)\n",
        "retriever = VectorIndexRetriever(index=index, similarity_top_k=1)"
      ],
      "metadata": {
        "id": "E3ibPaCk7yqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "q67gKmLF7_8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Chatbot Response Function\n",
        "# ========================\n",
        "def chatbot_response(query):\n",
        "    \"\"\"Retrieves context and generates an AI response.\"\"\"\n",
        "    response = retriever.retrieve(query)\n",
        "    context = \"\\n\".join([node.text for node in response])\n",
        "\n",
        "    # Create prompt with retrieved context\n",
        "    prompt = f\"\"\"[INST]EnigmaGPT, your AI study buddy, will answer based on the provided context:\n",
        "{context}\n",
        "Question: {query} [/INST]\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "GpBekivG80Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=chatbot_response,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me a question...\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"EnigmaBot\",\n",
        "    description=\"A Retrieval-Augmented Chatbot to answer questions based on given data.\"\n",
        ")"
      ],
      "metadata": {
        "id": "5A3JzeV488YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "CE8PZuxy897S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}